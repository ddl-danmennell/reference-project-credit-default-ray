{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Training with MLflow Integration\n",
    "\n",
    "This notebook extends the original credit risk prediction model with MLflow tracking capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import ray\n",
    "import glob\n",
    "import eli5\n",
    "import mlflow\n",
    "import xgboost_ray as xgbr\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from ray import tune\n",
    "\n",
    "# Set up MLflow tracking URI - replace with your MLflow server URL\n",
    "#mlflow.set_tracking_uri('http://your-mlflow-server:5000')\n",
    "#mlflow.set_experiment('credit-risk-prediction')\n",
    "\n",
    "# Enable MLflow autologging for XGBoost\n",
    "mlflow.xgboost.autolog()\n",
    "\n",
    "DATA_ROOT = os.path.join(\"/mnt/data\", os.environ[\"DOMINO_PROJECT_NAME\"], \"data\") \n",
    "MODEL_ROOT = \"/mnt/artifacts\"\n",
    "TUNE_ROOT = os.path.join(\"/mnt/data\", os.environ[\"DOMINO_PROJECT_NAME\"], \"ray_results\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ray setup\n",
    "RAY_ACTORS = 3\n",
    "RAY_CPUS_PER_ACTOR = 4\n",
    "\n",
    "if ray.is_initialized() == False:\n",
    "    service_host = os.environ[\"RAY_HEAD_SERVICE_HOST\"]\n",
    "    service_port = os.environ[\"RAY_HEAD_SERVICE_PORT\"]\n",
    "    ray.init(f\"ray://{service_host}:{service_port}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_files = glob.glob(os.path.join(DATA_ROOT, \"train_data*\"))\n",
    "val_files = glob.glob(os.path.join(DATA_ROOT, \"validation_data*\"))\n",
    "test_file = os.path.join(DATA_ROOT, \"test_data.csv\")\n",
    "target_col = \"credit\"\n",
    "\n",
    "rdm_train = xgbr.RayDMatrix(train_files, label=target_col)\n",
    "rdm_val = xgbr.RayDMatrix(val_files, label=target_col)\n",
    "df_test = pd.read_csv(test_file)\n",
    "rdm_test = xgbr.RayDMatrix(df_test, label=target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initial Model Training with MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='initial_model') as run:\n",
    "    param = {\n",
    "        \"seed\": 1234,\n",
    "        \"max_depth\": 3,\n",
    "        \"eta\": 0.1,\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"eval_metric\": [\"logloss\", \"error\"]\n",
    "    }\n",
    "    \n",
    "    mlflow.log_params(param)\n",
    "    \n",
    "    xgb_ray_params = xgbr.RayParams(\n",
    "        num_actors=RAY_ACTORS,\n",
    "        cpus_per_actor=RAY_CPUS_PER_ACTOR\n",
    "    )\n",
    "    \n",
    "    evals_result = {}\n",
    "    bst = xgbr.train(\n",
    "        param,\n",
    "        rdm_train,\n",
    "        num_boost_round=50,\n",
    "        verbose_eval=True,\n",
    "        evals_result=evals_result,\n",
    "        evals=[(rdm_train, \"train\"), (rdm_val, \"val\")],\n",
    "        ray_params=xgb_ray_params\n",
    "    )\n",
    "    \n",
    "    mlflow.log_metric(\"train_error\", evals_result[\"train\"][\"error\"][-1])\n",
    "    mlflow.log_metric(\"val_error\", evals_result[\"val\"][\"error\"][-1])\n",
    "    \n",
    "    print(f\"Final training error: {evals_result['train']['error'][-1]:.4f}\")\n",
    "    print(f\"Final validation error: {evals_result['val']['error'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning with MLflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"seed\": 1234,\n",
    "    \"eta\": tune.loguniform(3e-3, 3e-1),\n",
    "    \"max_depth\": tune.randint(2, 6),\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"eval_metric\": [\"logloss\", \"error\"]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_trainer(config):\n",
    "    with mlflow.start_run(nested=True) as run:\n",
    "        mlflow.log_params(config)\n",
    "        \n",
    "        evals_result = {}\n",
    "        bst = xgbr.train(\n",
    "            params=config,\n",
    "            dtrain=rdm_train,\n",
    "            num_boost_round=50,\n",
    "            evals_result=evals_result,\n",
    "            evals=[(rdm_train, \"train\"), (rdm_val, \"val\")],\n",
    "            ray_params=xgb_ray_params\n",
    "        )\n",
    "        \n",
    "        mlflow.log_metric(\"train_error\", evals_result[\"train\"][\"error\"][-1])\n",
    "        mlflow.log_metric(\"val_error\", evals_result[\"val\"][\"error\"][-1])\n",
    "        \n",
    "        bst.save_model(\"model.xgb\")\n",
    "        mlflow.log_artifact(\"model.xgb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='hyperparameter_tuning') as run:\n",
    "    analysis = tune.run(\n",
    "        my_trainer,\n",
    "        config=config,\n",
    "        resources_per_trial=xgb_ray_params.get_tune_resources(),\n",
    "        local_dir=TUNE_ROOT,\n",
    "        metric=\"val-error\",\n",
    "        mode=\"min\",\n",
    "        num_samples=10,\n",
    "        verbose=1,\n",
    "        progress_reporter=tune.JupyterNotebookReporter(overwrite=True)\n",
    "    )\n",
    "    \n",
    "    mlflow.log_params({\"best_\" + k: v for k, v in analysis.best_config.items()})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Model Evaluation with MLflow Tracking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with mlflow.start_run(run_name='final_model_evaluation') as run:\n",
    "    bst = xgb.Booster(model_file=os.path.join(MODEL_ROOT, \"tune_best.xgb\"))\n",
    "    mlflow.log_artifact(os.path.join(MODEL_ROOT, \"tune_best.xgb\"))\n",
    "    \n",
    "    xgb_ray_params = xgbr.RayParams(\n",
    "        num_actors=RAY_ACTORS,\n",
    "        cpus_per_actor=RAY_CPUS_PER_ACTOR\n",
    "    )\n",
    "    \n",
    "    predictions = xgbr.predict(bst, rdm_test, ray_params=xgb_ray_params)\n",
    "    pred_class = (predictions > 0.5).astype(\"int\")\n",
    "    actuals = df_test[target_col]\n",
    "    \n",
    "    accuracy = accuracy_score(pred_class, actuals)\n",
    "    precision = precision_score(pred_class, actuals)\n",
    "    recall = recall_score(pred_class, actuals)\n",
    "    f1 = f1_score(pred_class, actuals)\n",
    "    \n",
    "    mlflow.log_metrics({\n",
    "        \"test_accuracy\": accuracy,\n",
    "        \"test_precision\": precision,\n",
    "        \"test_recall\": recall,\n",
    "        \"test_f1\": f1\n",
    "    })\n",
    "    \n",
    "    # Log feature importance plot\n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    xgb.plot_importance(bst, importance_type=\"gain\", max_num_features=10, ax=ax)\n",
    "    plt.title(\"Feature Importance (Gain)\")\n",
    "    mlflow.log_figure(fig, \"feature_importance.png\")\n",
    "    plt.close()\n",
    "    \n",
    "    # Log model to MLflow model registry\n",
    "    mlflow.xgboost.log_model(bst, \"model\")\n",
    "    \n",
    "    print(f\"Accuracy on test: {accuracy:.2f}\")\n",
    "    print(f\"Precision on test: {precision:.2f}\")\n",
    "    print(f\"Recall on test: {recall:.2f}\")\n",
    "    print(f\"F1 score on test: {f1:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
