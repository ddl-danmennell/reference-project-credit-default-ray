{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3b9d3278-daef-41ae-8120-c085ec0e5029",
   "metadata": {},
   "source": [
    "# Data Generation\n",
    "\n",
    "This notebook uses the [Statlog (German Credit Data) Data Set](https://archive.ics.uci.edu/ml/datasets/statlog+(german+credit+data)) to prepare training and test datasets for the demo.\n",
    "\n",
    "It applies SMOTE and general upsampling to balance the dataset and to create a training set of a challenging size. No data pre-processing is done here besides applying proper column names to the dataset and recoding the target variable from {1,2} to {0,1}. This has no impact on the model performance or its metrics.\n",
    "\n",
    "We start by loading all the Python libraries needed for processing and generating the training/test data. Note, that we are leveraging [Domino datasets](https://docs.dominodatalab.com/en/latest/admin_guide/ae1654/domino-datasets/) as a data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499e3169-c697-430f-92ec-a91a7f826408",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pip install imblearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8164b4b6-8dc0-485c-bf86-9cf9b7923d9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "from imblearn.over_sampling import SMOTEN\n",
    "from sklearn.preprocessing import StandardScaler \n",
    "from collections import Counter\n",
    "\n",
    "# Set for reprudicibility\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Setup paths and filenames\n",
    "DATA_PATH = \"/mnt/data/\" + os.environ.get(\"DOMINO_PROJECT_NAME\") # Location of the Credit Card Dataset\n",
    "DATA_FILE = DATA_PATH + \"/german.data\"\n",
    "TRAINING_SET = DATA_PATH + \"/data/train_data.csv\"\n",
    "TEST_SET = DATA_PATH + \"/data/test_data.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dae8be95-6eff-47a0-bfa0-bae193737987",
   "metadata": {},
   "source": [
    "First, we make sure the raw data is available in the specified data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d5bbf7e-a2d6-459f-8475-96ead8a41aed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the dataset (if not already present)\n",
    "if not os.path.isfile(DATA_FILE):\n",
    "    urllib.request.urlretrieve((\"https://archive.ics.uci.edu/ml/machine-learning-databases\"\n",
    "                                \"/statlog/german/german.data\"), DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e721aaf-04b9-4fa4-a00f-769c2f1c2992",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Create the /data folder in the dataset if it hasn't already been created.\n",
    "if not os.path.isdir(DATA_PATH + \"/data\"):\n",
    "    os.makedirs(DATA_PATH + \"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443fb7e-7044-4c95-b72e-3dc16dcad575",
   "metadata": {},
   "source": [
    "Let's now introduce meaningful attribute names. We'll also re-map the class labels to {1,0}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b076ec-82b9-49ac-9982-c79319cd489f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Add human-readable column names\n",
    "col_names = names = [\"checking_account\", \"duration\", \"credit_history\", \"purpose\", \"credit_amount\", \n",
    "                     \"savings\", \"employment_since\", \"installment_rate\", \"status\", \"debtors_guarantors\", \n",
    "                     \"residence\", \"property\", \"age\", \"other_installments\", \"housing\", \n",
    "                     \"credits\", \"job\", \"dependents\", \"telephone\", \"foreign_worker\", \"credit\"]\n",
    "\n",
    "data_df = pd.read_csv(DATA_FILE,names = col_names, delimiter=\" \")\n",
    "\n",
    "# Remap the target attribute: 1 - good credit, 0 - bad credit\n",
    "data_df[\"credit\"].replace([1,2], [1,0], inplace=True)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f0acf08-45c1-4805-90a5-9c77a14d17a1",
   "metadata": {},
   "source": [
    "Next, we need to unpack the categorical attributes using indicator variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66beb13c-278d-48ed-9029-ebcc4b6f05bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "num_attr_names = [\"duration\", \"credit_amount\", \"installment_rate\", \"residence\", \n",
    "                  \"age\", \"credits\", \"dependents\"]\n",
    "cat_attr_names = [\"checking_account\", \"credit_history\", \"purpose\", \"savings\", \"employment_since\", \n",
    "                  \"status\", \"debtors_guarantors\", \"property\", \"other_installments\", \"housing\", \n",
    "                  \"job\", \"telephone\", \"foreign_worker\"]\n",
    "\n",
    "dummies = pd.get_dummies(data_df[cat_attr_names])\n",
    "data_df = data_df.drop(cat_attr_names,axis = 1)\n",
    "data_df = pd.concat([data_df, dummies], axis=1)\n",
    "\n",
    "data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d4be841-f1df-4ab4-b074-12258230a42f",
   "metadata": {},
   "source": [
    "We'll also transform all numerical features features by scaling them to (0,1)\n",
    "\n",
    "Note, that this type of transformation leaks information about the mean into the test set. We decide to ignore this as we are performing more of a functional demo than rigorous modelling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995fe7fa-0035-4e95-b6a6-8d78380cba19",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "data_df[num_attr_names] = scaler.fit_transform(data_df[num_attr_names])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18b1d1a4-eeda-412b-9503-f0638ecc6541",
   "metadata": {},
   "source": [
    "We now split the dataset into training and test subset using a ratio of 80:20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9efb672b-cba4-4842-9f9d-db044edd1b05",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Retain 20% of the data for test\n",
    "mask = np.random.rand(len(data_df)) < 0.8\n",
    "\n",
    "train_df = data_df[mask]\n",
    "\n",
    "test_df = data_df[~mask]\n",
    "\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f0b509-99e6-4a52-846d-8f2b56aec55b",
   "metadata": {},
   "source": [
    "The test set is ready. We'll now save it and exclude it from any further transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8686de30-9c3d-4599-80d9-e2de1d69bf33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save the test set\n",
    "test_df.to_csv(TEST_SET, sep=\",\", header=True, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7403282b-9f73-4a84-939e-c57721e0b641",
   "metadata": {},
   "source": [
    "## Data balancing and generation\n",
    "\n",
    "First, let's check the class balance of the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a7467a-bf84-434e-a4c0-cd9befda0d9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X = train_df.loc[:, train_df.columns != \"credit\"]\n",
    "y = train_df[\"credit\"]\n",
    "\n",
    "print(\"Original class counts: {}\".format(Counter(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efd79a7-21c9-4acc-aaf7-b2fb5cb6d9db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "value_counts = y.value_counts()\n",
    "value_counts.plot(kind=\"bar\", title=\"Class distribution of the target variable\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f84469-177b-4eaf-aef2-324f4c001187",
   "metadata": {},
   "source": [
    "We see that the training dataset is imbalanced. Moreover, the dataset is relatively small. We'll balance it and upsample the data in order to make this a more challenging problem.\n",
    "\n",
    "We'll use the following recipe to generate the new training set:\n",
    "\n",
    "1. Save the original training dataset in `training_set`. We do this to preserve all of the original training data. We are not upsampling this set on purpose as to prevent the leakage of synthetic data into the upsampled chunks that follow.\n",
    "\n",
    "Repeat for N iterations:\n",
    "\n",
    "    1. Randomly sample 300 observations from the training subset\n",
    "    2. Use [SMOTEN](https://imbalanced-learn.org/stable/references/generated/imblearn.over_sampling.SMOTEN.html) to balance the data\n",
    "    3. Append the resulting balanced partition to `training_set`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3083b0d-b79a-4479-aac8-20a3252ff0fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def generate(n_chunks, chunk_size, file_name_prefix):\n",
    "\n",
    "    for chunk in range(n_chunks):\n",
    "\n",
    "        print(\"Generating chunk {}\".format(chunk))\n",
    "        df_chunk = pd.DataFrame().reindex(columns=X.columns)\n",
    "\n",
    "        while (df_chunk.shape[0] < chunk_size):\n",
    "            X_sample = X.sample(300, random_state=chunk)\n",
    "            y_sample = y[X_sample.index]\n",
    "\n",
    "            sampler = SMOTEN(random_state=chunk)\n",
    "            X_balanced, y_balanced = sampler.fit_resample(X_sample, y_sample)\n",
    "\n",
    "            df_chunk = pd.concat([df_chunk, pd.concat([X_balanced, y_balanced], axis=1)], axis=0)\n",
    "\n",
    "\n",
    "        df_chunk[\"credit\"] =df_chunk[\"credit\"].astype(int)\n",
    "        df_chunk.head(chunk_size).to_csv(file_name_prefix + str(chunk) + \".csv\", sep=\",\", header=True, index=False)\n",
    "        \n",
    "    print(\"Number of samples in the generated set is: {:,}\".format(n_chunks * chunk_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42f17fd6-765b-4895-a423-fc492b2fab89",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Generating training data using oversampling and SMOTE.\")\n",
    "\n",
    "# Generate training data\n",
    "generate(21, 100000, DATA_PATH + \"/data/train_data_\")\n",
    "\n",
    "print(\"Generating validation data using oversampling and SMOTE.\")\n",
    "\n",
    "# Generate validation data\n",
    "generate(3, 100000, DATA_PATH + \"/data/validation_data_\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe33c73a-63bb-4929-8f26-166b2d528c77",
   "metadata": {},
   "source": [
    "This concludes the data preparation aspect of the project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23fe1f44-8c65-482f-8d3a-e0c7effecab5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "dca-init": "true",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
